basic model - fine-tuned every task separately, no scheduler results on dev: sts - 0.277 | sst - 0.411 | para - 0.701
scheduler effect - same as base but with scheduler with reduce on plateau 0.1 every 4 with 20 epochs - sts - 0.262 | sst - 0.416 | para - DNE
loss function for sts - changed from MSE to L1 loss - sts - 0.256
                        MSE + 5 * sigmoid  - sts - 0.274
                        hubber_loss + 5 * sigmoid - 0.275
STS prediction - dot product instead of linear layer (32) - L1 0.308
                                                     (16) - L1 0.285
                                                     (16) - hub 0.275
                                                     (16) - MSE 0.279
                 dot product could be negative - sigmoid - not stable
                                               - abs - 0.255 but started to converge only after 10 epochs
                                               - not multiply by 5 - 0.297
                 cosine similarity with sigmoid (16) - 0.251
                                                (32) - 0.246
                                                (128) - 0.237

full model fine tune - sts - 0.332 | sst - 0.517 | para - 0.810, needs to split to 3 different models
there was no effect to train more then 7 epochs on full model fine tune.
Cosine embedding loss with full model train and then fine tune only the new layers : sts(dot product) - 0.379 | sst - 0.417 | para - 0.710
                                                                                     sts(cosine similarity) - 0.453 | sst - 0.409 | para 0.706
Add a interim layer for the bert model - sts - 0.454 | sst - 0.282 | para - 0.640
Change the bert output, from poller to average of the last layer - sts - 0.588 | sst - 0.396 | para - 0.690
Concatenate poller output with the average last hidden state and feed forward - sts - 0.461 | sst - 0.442 | para - 0.717
Full train cosine embedding loss with last layer, then full train on the pooler output, then fine tune - sts - 0.530 | sst - 0.385 | para - 0.701